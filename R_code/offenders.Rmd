---
title: "How Likey are you to get away with murderr "
output: html_notebook
---
---
title: "Classfication_how likly are you to be murdered"
output: html_notebook
---

AIM : How likely are you to be the  murder?

```{r}
library(dplyr)
library(glmnet)
library(pROC)

# Read the CSV file
murder_data <- read.csv("SUP1.csv")

# Generate index values
index <- seq_len(nrow(murder_data))

# Insert index column into the dataset
murder_data$Index <- index

selected_columns <- c("State",  
                      "Solved", "OffAge", "OffSex", "OffRace", "OffEthnic")


murder_data$OffRace <- as.factor(murder_data$OffRace)
murder_data$OffEthnic <- as.factor(murder_data$OffEthnic)

# Convert 'VicRace' to a factor with specified levels and corresponding numerical values
murder_data$OffRace <- factor(murder_data$OffRace, levels = c("American Indian or Alaskan Native", 
                                                              "Black", 
                                                              "White", 
                                                              "Unknown", 
                                                              "Asian", 
                                                              "Native Hawaiian or Pacific Islander"),
                               labels = c(1, 2, 3, 4, 5, 6))


# Convert 'VicSex' to numeric
murder_data$OffSex <- ifelse(murder_data$OffSex == "Female", 1,
                            ifelse(murder_data$OffSex == "Male", 0, -1))



# Convert 'VicRace' to integer (assuming it's already a factor)
murder_data$OffRace <- as.integer(as.character(murder_data$OffRace))

# Convert 'VicEthnic' to numeric
murder_data$OffEthnic <- ifelse(murder_data$OffEthnic == "Not of Hispanic origin", 1,
                                 ifelse(murder_data$OffEthnic == "Hispanic", 0, -1))

# Convert "Yes" to 1 and "No" to 0 in the 'Solved' column
murder_data$Solved <- ifelse(murder_data$Solved == "Yes", 1, 0)

# Define a mapping of state names to numeric identifiers
state_mapping <- c(
  "Alabama" = 1, "Alaska" = 2, "Arizona" = 3, "Arkansas" = 4, "California" = 5,
  "Colorado" = 6, "Connecticut" = 7, "Delaware" = 8, "Florida" = 9, "Georgia" = 10,
  "Hawaii" = 11, "Idaho" = 12, "Illinois" = 13, "Indiana" = 14, "Iowa" = 15,
  "Kansas" = 16, "Kentucky" = 17, "Louisiana" = 18, "Maine" = 19, "Maryland" = 20,
  "Massachusetts" = 21, "Michigan" = 22, "Minnesota" = 23, "Mississippi" = 24, "Missouri" = 25,
  "Montana" = 26, "Nebraska" = 27, "Nevada" = 28, "New Hampshire" = 29, "New Jersey" = 30,
  "New Mexico" = 31, "New York" = 32, "North Carolina" = 33, "North Dakota" = 34, "Ohio" = 35,
  "Oklahoma" = 36, "Oregon" = 37, "Pennsylvania" = 38, "Rhode Island" = 39, "South Carolina" = 40,
  "South Dakota" = 41, "Tennessee" = 42, "Texas" = 43, "Utah" = 44, "Vermont" = 45,
  "Virginia" = 46, "Washington" = 47, "West Virginia" = 48, "Wisconsin" = 49, "Wyoming" = 50, "District of Columbia" = 51
)

# Convert state names to numeric identifiers in the 'State' column
murder_data$State <- state_mapping[as.character(murder_data$State)]

# Select the desired columns
murder_data_selected <- murder_data[, selected_columns]

# Perform train-test split (80/20)
set.seed(4)  # for reproducibility
train_index <- sample(1:nrow(murder_data_selected), 0.8 * nrow(murder_data_selected))
train_data <- murder_data_selected[train_index, ]
test_data <- murder_data_selected[-train_index, ]





```

##Logistic Regression

```{r}


# Fit logistic regression model
logit_model <- glm(Solved ~ ., data = train_data, family = binomial)

# Make predictions on test data
test_predictions <- predict(logit_model, newdata = test_data, type = "response")

# Evaluate the predictions
conf_matrix <- table(round(test_predictions), test_data$Solved)
print(conf_matrix)

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
print(summary(logit_model))
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))

```



# Naye Bayes

```{r}
# Install and load the necessary package
library(e1071)

features <- subset(train_data, select = c("OffAge", "OffRace","State","OffSex"))

#'Solved' is the target variable
target <- train_data$Solved

# Create and train the Naive Bayes model
naive_bayes_model <- naiveBayes(features, target)


test_features <- subset(test_data, select = c("OffAge", "OffRace","State","OffSex"))

# Use the trained model to make predictions on the test dataset
predictions <- predict(naive_bayes_model, test_features)

# Print the predictions
confusion_matrix <- table(predictions, test_data$Solved)
print(confusion_matrix)
accuracy <- sum(predictions == test_data$Solved) / length(predictions)
print(paste("Accuracy:", round(accuracy, 2)))


precision <- sum(predictions == 1 & test_data$Solved == 1) / sum(predictions == 1)
recall <- sum(predictions == 1 & test_data$Solved == 1) / sum(test_data$Solved == 1)
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1-Score:", round(f1_score, 2)))


```



# Decison Trees

```{r}
# Load required library
library(rpart)
library(rpart.plot)

features <- subset(train_data, select = c("OffAge", "OffRace","State","OffSex"))
target <- train_data$Solved
# Create and train the Decision Tree model with adjusted cp parameter
tree_model <- rpart(Solved ~ ., data = train_data, method = "class", control = rpart.control(cp = 0.1))

# Print summary of the Decision Tree model
summary(tree_model)

# Plot the Decision Tree
plot(tree_model)
text(tree_model, cex = 0.8)


# Print summary of the Decision Tree model
summary(tree_model)

# Plot the Decision Tree

# Plot the decision tree
rpart.plot(tree_model, box.palette = "RdBu", shadow.col = "gray", nn = TRUE)
#plot(tree_model)
#text(tree_model, cex = 0.8)
```



# Gradient Boosting Machine 

```{r}
# Load the required library
library(gbm)

features <- subset(train_data, select = c("OffAge", "OffRace","State","OffSex"))
target <- train_data$Solved

# Train the GBM model
gbm_model <- gbm(target ~ ., data = train_data, distribution = "bernoulli", n.trees = 50, interaction.depth = 3, shrinkage = 0.1)

# Print the summary of the GBM model
summary(gbm_model)

# Predictions on test data
predictions <- predict(gbm_model, newdata = test_data, type = "response")

```

```{r}
# Load the required library
library(gbm)

features <- subset(train_data, select = c("OffAge", "OffRace","State","OffSex"))
target <- train_data$Solved

# Train the GBM model
gbm_model <- gbm(target ~ ., data = train_data, distribution = "bernoulli", n.trees = 50, interaction.depth = 3, shrinkage = 0.1)

# Print the summary of the GBM model
summary(gbm_model)

# Predictions on test data
predictions <- predict(gbm_model, newdata = test_data, type = "response")

```
# KNN

```{r}

library(class)

# Remove rows with missing values
train_data <- na.omit(train_data)
test_data <- na.omit(test_data)

  # Train the KNN model
  k <- 5  # Define the number of neighbors
  knn_model <- knn(train = train_data[, -which(names(train_data) == "Solved")], 
                   test = test_data[, -which(names(test_data) == "Solved")], 
                   cl = train_data$Solved, k = k)

  # Model Evaluation Metrics
  confusion_matrix <- table(knn_model, test_data$Solved)
  print(confusion_matrix)

  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
  recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
  f1_score <- 2 * precision * recall / (precision + recall)

  print(paste("Accuracy:", accuracy))
  print(paste("Precision:", precision))
  print(paste("Recall:", recall))
  print(paste("F1-Score:", f1_score))



```

